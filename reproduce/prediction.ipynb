{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理：\n",
    "三个版本：一个是基于mat2vec的预测，一个是基于自己训练的vec的预测，一个是基于mat2vec+继续训练的预测  \n",
    "方法：和self-cleaning近，以及hydrophobic等性质近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_text(dir_name):\n",
    "    text_dic = {}    \n",
    "    elsevier = os.listdir(dir_name+'/')\n",
    "    for e in tqdm(elsevier):\n",
    "        with open(dir_name+'/'+e, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            doi = data[0][:-1]\n",
    "            start_words = ['Graphical abstract','Abstract', 'Corresponding author', 'Correspondence to:', 'Introduction', 'Keywords']\n",
    "            s_find = 0\n",
    "            for s in start_words:\n",
    "                if s in data[1]:\n",
    "                    start = data[1].find(s)\n",
    "                    s_find = 1\n",
    "            if s_find == 0:\n",
    "                start = 0\n",
    "            if 'Reference' in data[1]:\n",
    "                end = data[1].rfind('Reference')\n",
    "            else:\n",
    "                end = len(data[1])\n",
    "            fulltext = data[1][start:end]                \n",
    "            text_dic[e]={}\n",
    "            text_dic[e]['text']=fulltext\n",
    "            text_dic[e]['doi']=doi\n",
    "    return text_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mat2vec版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Anti-soiling_Elsevier_53\n",
    "# Antistatic_Elsevier_156\n",
    "# Hydrophilic_Elsevier_8921\n",
    "# Hydrophobic_Elsevier_13677\n",
    "# Oleophobic_Elsevier_581\n",
    "# Omniphobic_Elsevier_143\n",
    "# Photocatalytic_Elsevier_13437\n",
    "# SC_Elesever_2044\n",
    "model = Word2Vec.load('mat2vec/training/models/pretrained_embeddings')\n",
    "word_vectors = model.wv\n",
    "# 因为词汇里没有self-cleaning, 所以用avg(self+clean)取代\n",
    "# 因为词汇里没有anti-soiling, 所以用avg(anti+soiling)取代\n",
    "proper = ['soiling', 'antistatic', 'hydrophilic', 'hydrophobic', 'oleophobic', 'omniphobic', 'photocatalytic']\n",
    "self = word_vectors['self']\n",
    "cleaning = word_vectors['cleaning']\n",
    "vec1 = (self+cleaning)/2\n",
    "similar1 = word_vectors.most_similar(positive=[vec1], topn=5000)\n",
    "store = {}\n",
    "for p in proper:\n",
    "    if p == 'soiling':\n",
    "        anti = word_vectors['anti']\n",
    "        soiling = word_vectors['soiling']\n",
    "        vec2 = (anti+soiling) / 2\n",
    "        name = 'anti-soiling'\n",
    "    else:\n",
    "        vec2 = word_vectors[p]\n",
    "        name = p\n",
    "    store[name] = {}\n",
    "    similar2 = word_vectors.most_similar(positive=[vec2], topn=5000)\n",
    "    for i, s2 in enumerate(similar2):\n",
    "        for j, s1 in enumerate(similar1):\n",
    "            if s2[0] == s1[0]:\n",
    "                store[name][s2[0]] = (i+j)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2044/2044 [00:01<00:00, 1511.82it/s]\n",
      "100%|██████████| 2044/2044 [30:33<00:00,  1.11it/s] \n"
     ]
    }
   ],
   "source": [
    "from mat2vec.processing import MaterialsTextProcessor\n",
    "import nltk\n",
    "\n",
    "text_processor = MaterialsTextProcessor()\n",
    "dir_name = 'SC_Elesever_2044'\n",
    "ol_dict = get_text(dir_name)\n",
    "voc = []\n",
    "for o in tqdm(ol_dict.keys()):\n",
    "    text = ol_dict[o]['text']\n",
    "    sens = nltk.sent_tokenize(text)\n",
    "    for s in sens:\n",
    "        processed, _ = text_processor.process(s)\n",
    "        # print(processed)\n",
    "        for p in processed:\n",
    "            if p not in voc:\n",
    "                voc.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in store.keys():\n",
    "    with open('prediction/mat2vec/'+s+'_prediction.txt', 'w', encoding='utf-8') as f:\n",
    "        sorted_store = sorted(store[s].items(), key=lambda item:item[1])\n",
    "        for ss in sorted_store:\n",
    "            if ss[0] in voc:\n",
    "                judge = 1\n",
    "            else:\n",
    "                judge = 0\n",
    "            f.write(ss[0]+'\\t'+str(ss[1])+'\\t'+str(judge)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新训练版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('final_pairs.json', 'r', encoding='utf-8') as file:\n",
    "    final_pairs = json.load(file)\n",
    "full_ab = {}\n",
    "for f in final_pairs.keys():\n",
    "    for full in final_pairs[f].keys():\n",
    "        full_ab[full] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def simple(words):\n",
    "    simplified = []\n",
    "    for w in words:\n",
    "        if len(w) == 1:\n",
    "            if w not in string.punctuation:\n",
    "                simplified.append(w)\n",
    "        else:\n",
    "            if w not in stop: \n",
    "                simplified.append(w)\n",
    "    return simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 669.66it/s]\n",
      "100%|██████████| 52/52 [00:02<00:00, 20.01it/s]\n",
      "100%|██████████| 155/155 [00:00<00:00, 898.04it/s]\n",
      "100%|██████████| 155/155 [00:06<00:00, 25.24it/s]\n",
      "100%|██████████| 8921/8921 [00:10<00:00, 870.91it/s] \n",
      "100%|██████████| 8921/8921 [13:23<00:00, 11.10it/s]\n",
      "100%|██████████| 13677/13677 [00:15<00:00, 875.55it/s]\n",
      "100%|██████████| 13677/13677 [24:05<00:00,  9.46it/s]\n",
      "100%|██████████| 581/581 [00:00<00:00, 864.74it/s]\n",
      "100%|██████████| 581/581 [00:30<00:00, 18.82it/s]\n",
      "100%|██████████| 143/143 [00:00<00:00, 688.63it/s]\n",
      "100%|██████████| 143/143 [00:07<00:00, 20.00it/s]\n",
      "100%|██████████| 13405/13405 [00:15<00:00, 891.81it/s] \n",
      "100%|██████████| 13405/13405 [22:38<00:00,  9.87it/s] \n",
      "100%|██████████| 2044/2044 [00:02<00:00, 908.45it/s]\n",
      "100%|██████████| 2044/2044 [50:42<00:00,  1.49s/it]  \n"
     ]
    }
   ],
   "source": [
    "# 收集所有文本，把里面material branch替换成main; 把分开的两个词合起来\n",
    "all_sens = []  \n",
    "sc_voc = []\n",
    "all_materials = []\n",
    "for topic in os.listdir('material_names'):\n",
    "    materials = []\n",
    "    with open('material_names/'+topic+'/sen_dict.json', 'r', encoding='utf-8') as f:\n",
    "        mat_sens = json.load(f)\n",
    "        ol_dict = get_text(topic)\n",
    "        for o in tqdm(ol_dict.keys()):\n",
    "            text = ol_dict[o]['text']\n",
    "            sens = nltk.sent_tokenize(text)\n",
    "            for s in sens:\n",
    "                if s in mat_sens.keys():\n",
    "                    # print(s)\n",
    "                    tmp = []\n",
    "                    starts = []\n",
    "                    ends = []\n",
    "                    tmp_st = 0\n",
    "                    for m in mat_sens[s]['materials'].keys():\n",
    "                        starts.append(mat_sens[s]['materials'][m][0])\n",
    "                        ends.append(mat_sens[s]['materials'][m][1])\n",
    "                        if m in full_ab.keys():\n",
    "                            if full_ab[m] not in materials:\n",
    "                                materials.append(full_ab[m])\n",
    "                        else:\n",
    "                            if m not in materials:\n",
    "                                materials.append(m)   \n",
    "                    starts.sort()\n",
    "                    ends.sort()\n",
    "                    for i, st in enumerate(starts):\n",
    "                        split = nltk.word_tokenize(s[tmp_st:st])\n",
    "                        # print(split)\n",
    "                        tmp.extend(split)\n",
    "                        if s[st:ends[i]] in full_ab.keys():\n",
    "                            tmp.append(full_ab[s[st:ends[i]]])\n",
    "                            # print(full_ab[s[st:ends[i]]])\n",
    "                        else:\n",
    "                            tmp.append(s[st:ends[i]])\n",
    "                            # print(s[st:ends[i]])\n",
    "                        tmp_st = ends[i]\n",
    "                    # print(nltk.word_tokenize(s[tmp_st:]))\n",
    "                    tmp.extend(nltk.word_tokenize(s[tmp_st:]))\n",
    "                    sim_tmp = simple(tmp)\n",
    "                    all_sens.append(sim_tmp)\n",
    "                    if topic == 'SC_Elesever_2044':\n",
    "                        for st in sim_tmp:\n",
    "                            if st not in sc_voc:\n",
    "                                sc_voc.append(st)\n",
    "                    # print(sim_tmp)\n",
    "                else:\n",
    "                    split = nltk.word_tokenize(s)\n",
    "                    sim_split = simple(split)\n",
    "                    all_sens.append(sim_split)\n",
    "                    if topic == 'SC_Elesever_2044':\n",
    "                        for st in sim_split:\n",
    "                            if st not in sc_voc:\n",
    "                                sc_voc.append(st)\n",
    "    all_materials.extend(materials)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2516/2516 [00:00<?, ?it/s]\n",
      "100%|██████████| 10594/10594 [00:00<00:00, 706127.05it/s]\n",
      "100%|██████████| 676568/676568 [00:01<00:00, 486521.31it/s]\n",
      "100%|██████████| 1023296/1023296 [00:02<00:00, 482379.17it/s]\n",
      "100%|██████████| 42823/42823 [00:00<00:00, 552858.53it/s]\n",
      "100%|██████████| 11784/11784 [00:00<00:00, 743153.88it/s]\n",
      "100%|██████████| 1350480/1350480 [00:02<00:00, 631072.21it/s]\n",
      "100%|██████████| 163769/163769 [00:00<00:00, 666588.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_materials = []\n",
    "for topic in os.listdir('material_names'):\n",
    "    materials = []\n",
    "    with open('material_names/'+topic+'/sen_dict.json', 'r', encoding='utf-8') as f:\n",
    "        mat_sens = json.load(f)\n",
    "        for s in tqdm(mat_sens.keys()):\n",
    "            for m in mat_sens[s]['materials'].keys():\n",
    "                if m in full_ab.keys():\n",
    "                    if full_ab[m] not in materials:\n",
    "                        materials.append(full_ab[m])\n",
    "                    else:\n",
    "                        if m not in materials:\n",
    "                            materials.append(m)\n",
    "    all_materials.extend(materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226599\n"
     ]
    }
   ],
   "source": [
    "tmp = list(set(all_materials))\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8036846/8036846 [02:02<00:00, 65413.79it/s] \n"
     ]
    }
   ],
   "source": [
    "new_sens = []\n",
    "for als in tqdm(all_sens):\n",
    "    tmp = []\n",
    "    for w in als:\n",
    "        if w in change.keys():\n",
    "            tmp.append(change[w])\n",
    "        else:\n",
    "            tmp.append(w)\n",
    "    new_sens.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "json_str = json.dumps(new_sens, indent=4)\n",
    "with open('all_sens.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)\n",
    "json_str = json.dumps(new_vc, indent=4)\n",
    "with open('sc_voc.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)\n",
    "json_str = json.dumps(new_materials, indent=4)\n",
    "with open('all_materials.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)\n",
    "'''\n",
    "with open('all_sens.json', 'r', encoding='utf-8') as json_file:\n",
    "    all_sens = json.load(json_file)\n",
    "with open('sc_voc.json', 'r', encoding='utf-8') as json_file:\n",
    "    sc_voc = json.load(json_file)\n",
    "with open('all_materials.json', 'r', encoding='utf-8') as json_file:\n",
    "    all_materials = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "our_model = Word2Vec(all_sens, size=300, window=12, workers=4)\n",
    "our_model.save(\"our_embedding/word2vec_our.model\")\n",
    "# our_model = Word2Vec.load(\"our_embedding/word2vec_our.model\")\n",
    "endtime = datetime.datetime.now()\n",
    "print (endtime - starttime).seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:39, 255.45it/s]\n",
      "10000it [00:36, 271.96it/s]\n",
      "10000it [00:29, 343.10it/s]\n",
      "10000it [00:30, 323.06it/s]\n",
      "10000it [00:34, 292.34it/s]\n",
      "10000it [00:34, 286.28it/s]\n",
      "10000it [00:33, 295.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "word_vectors = our_model.wv\n",
    "# print(word_vectors.vocab['TiO2 [2–4]'].count)\n",
    "proper = ['anti-soiling', 'antistatic', 'hydrophilic', 'hydrophobic', 'oleophobic', 'omniphobic', 'photocatalytic']\n",
    "vec1 = word_vectors['self-cleaning']\n",
    "similar1 = word_vectors.most_similar(positive=[vec1], topn=10000)\n",
    "\n",
    "store = {}\n",
    "for p in proper:\n",
    "    vec2 = word_vectors[p]\n",
    "    store[p] = {}\n",
    "    similar2 = word_vectors.most_similar(positive=[vec2], topn=10000)\n",
    "    for i, s2 in tqdm(enumerate(similar2)):\n",
    "        for j, s1 in enumerate(similar1):\n",
    "            if s2[0] == s1[0] and s2[0] in all_materials:\n",
    "                store[p][s2[0]] = (i+j)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 325823/325823 [27:54<00:00, 194.59it/s] \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "new_materials = []\n",
    "change = {}\n",
    "for m in tqdm(all_materials):\n",
    "# for m in sc_voc:\n",
    "    if filt(m):\n",
    "        nm = filt2(m)\n",
    "        if nm!='delete':\n",
    "            if nm not in new_materials:\n",
    "                new_materials.append(nm)\n",
    "            if nm!=m:\n",
    "                change[m] = nm\n",
    "print(len(sc_voc))\n",
    "new_vc = []\n",
    "for m in sc_voc:\n",
    "    if m in change.keys():\n",
    "        new_vc.append(change[m])\n",
    "    else:\n",
    "        new_vc.append(m)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt(word):\n",
    "    contain_eng = 0\n",
    "    for c in word:\n",
    "        if c.isalpha():\n",
    "            contain_eng = 1\n",
    "    if contain_eng == 0 or ' or ' in word:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt2(word):\n",
    "    if '[' in word and ']' in word:\n",
    "        if word.index('[') > word.index(']'):\n",
    "            return 'delete'\n",
    "        else:\n",
    "            for_check = word[word.index('[')+1:word.index(']')]\n",
    "            if filt(for_check):\n",
    "                return word\n",
    "            else:\n",
    "                nc = word[word.index('['):word.index(']')+1]\n",
    "                new_word = word.replace(nc, '').strip()\n",
    "                return new_word\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for s in tqdm(store.keys()):\n",
    "    with open('prediction/ourvec/'+s+'_prediction.txt', 'w', encoding='utf-8') as f:\n",
    "        sorted_store = sorted(store[s].items(), key=lambda item:item[1])\n",
    "        for ss in sorted_store:\n",
    "            if ss[0] in sc_voc:\n",
    "                judge = 1\n",
    "            else:\n",
    "                judge = 0\n",
    "            f.write(ss[0]+'\\t'+str(ss[1])+'\\t'+str(judge)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 继续训练版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216096/216096 [00:36<00:00, 5914.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalize原来的材料词\n",
    "from mat2vec.processing import MaterialsTextProcessor\n",
    "\n",
    "text_processor = MaterialsTextProcessor()\n",
    "pro_materials = []\n",
    "pro = {}\n",
    "for m in tqdm(all_materials):\n",
    "    processed, _ = text_processor.process(m)\n",
    "    if len(processed)>1:\n",
    "        pro_materials.append(m)\n",
    "    else:\n",
    "        pro_materials.append(processed[0])\n",
    "        pro[m] = processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8036846/8036846 [00:49<00:00, 163091.21it/s]\n"
     ]
    }
   ],
   "source": [
    "pro_sens = []\n",
    "for als in tqdm(all_sens):\n",
    "    tmp = []\n",
    "    for w in als:\n",
    "        if w in pro.keys():\n",
    "            tmp.append(pro[w])\n",
    "        else:\n",
    "            tmp.append(w)\n",
    "    pro_sens.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Word2Vec.load(\"our_embedding/word2vec_our.model\")\n",
    "new_model.build_vocab(pro_sens, update=True)\n",
    "new_model.train(pro_sens,total_examples=new_model.corpus_count,epochs=1)\n",
    "new_vectors = new_model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:31, 321.61it/s]\n",
      "10000it [00:29, 333.54it/s]\n",
      "10000it [00:27, 369.31it/s]\n",
      "10000it [00:28, 350.07it/s]\n",
      "10000it [00:28, 348.23it/s]\n",
      "10000it [00:28, 348.19it/s]\n",
      "10000it [00:28, 355.37it/s]\n"
     ]
    }
   ],
   "source": [
    "proper = ['anti-soiling', 'antistatic', 'hydrophilic', 'hydrophobic', 'oleophobic', 'omniphobic', 'photocatalytic']\n",
    "vec1 = new_vectors['self-cleaning']\n",
    "similar1 = new_vectors.most_similar(positive=[vec1], topn=10000)\n",
    "\n",
    "store = {}\n",
    "for p in proper:\n",
    "    vec2 = new_vectors[p]\n",
    "    store[p] = {}\n",
    "    similar2 = new_vectors.most_similar(positive=[vec2], topn=10000)\n",
    "    for i, s2 in tqdm(enumerate(similar2)):\n",
    "        for j, s1 in enumerate(similar1):\n",
    "            if s2[0] == s1[0] and if_material(s2[0]):\n",
    "                store[p][s2[0]] = (i+j)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "change_pro={v:k for k, v in pro.items()}\n",
    "for s in tqdm(store.keys()):\n",
    "    with open('prediction/ourvec_keep/'+s+'_prediction.txt', 'w', encoding='utf-8') as f:\n",
    "        sorted_store = sorted(store[s].items(), key=lambda item:item[1])\n",
    "        for ss in sorted_store:\n",
    "            if ss[0] in change_pro.keys():\n",
    "                if change_pro[ss[0]] in sc_voc:\n",
    "                    judge = 1\n",
    "                else:\n",
    "                    judge = 0\n",
    "            else:\n",
    "                if ss[0] in sc_voc:\n",
    "                    judge = 1\n",
    "                else:\n",
    "                    judge = 0\n",
    "            f.write(ss[0]+'\\t'+str(ss[1])+'\\t'+str(judge)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemdataextractor import Document\n",
    "\n",
    "def if_material(word):\n",
    "    doc = Document(word)\n",
    "    if doc.cems:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(if_material('PDRC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
