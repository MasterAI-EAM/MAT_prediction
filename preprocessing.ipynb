{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac8b462",
   "metadata": {},
   "source": [
    "## Get full texts and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cad8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10321/10321 [13:00<00:00, 13.22it/s] \n"
     ]
    }
   ],
   "source": [
    "# 处理xml files为meta data的dict\n",
    "import os\n",
    "from chemdataextractor import Document\n",
    "from chemdataextractor.scrape import Selector\n",
    "from chemdataextractor.reader.markup import XmlReader\n",
    "from chemdataextractor.doc import Document, Title, Heading, Paragraph, Citation, Table, Figure, Caption, Footnote\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "xml_list = os.listdir('xml_files')\n",
    "xml_dict = {}\n",
    "for x in tqdm(xml_list):\n",
    "    f = open('xml_files/'+x, 'rb')\n",
    "    doc = Document.from_file(f)\n",
    "    headings = []\n",
    "    heading_idx = []\n",
    "    paras = []\n",
    "    para_idx = []\n",
    "    for i, e in enumerate(doc.elements):\n",
    "        if type(e) == Title:\n",
    "            str_title = str(e).replace('\\n', ' ')\n",
    "        if type(e) == Heading:\n",
    "            str_heading = str(e).replace('\\n', ' ')\n",
    "            headings.append(str_heading)\n",
    "            heading_idx.append(i)\n",
    "        if type(e) == Paragraph:\n",
    "            str_para = str(e).replace('\\n', ' ')\n",
    "            if len(str_para.split(' '))>50:\n",
    "                paras.append(str_para)\n",
    "                para_idx.append(i)\n",
    "    tree = ET.parse('xml_files/'+x)\n",
    "    root = tree.getroot()\n",
    "    x_doi = tree.find('./front/article-meta/article-id').text\n",
    "    xml_dict[x_doi] = {}\n",
    "    ab_find = tree.find('./front/article-meta/abstract')\n",
    "    if ab_find != None:\n",
    "        for node in ab_find:\n",
    "            if node.tag == 'p':\n",
    "                ab_s = \"\".join(t for t in node.itertext()).replace('\\n', ' ')\n",
    "                # 摘要\n",
    "                xml_dict[x_doi]['abstract'] = ab_s\n",
    "    time = []\n",
    "    for node in tree.find('./front/article-meta/history'):\n",
    "        if node.attrib['date-type']=='received':\n",
    "            time.append(int(node[2].text))\n",
    "            time.append(int(node[1].text))\n",
    "    # 时间\n",
    "    xml_dict[x_doi]['time'] = time\n",
    "    authors = []\n",
    "    for node in tree.find('./front/article-meta/contrib-group'):\n",
    "        if node.get('contrib-type') != None and node.attrib['contrib-type'] == 'author':\n",
    "            name_find = node.find('name')\n",
    "            if name_find != None:\n",
    "                single_name = ''\n",
    "                for n in name_find:\n",
    "                    single_name+=n.text+' '\n",
    "                authors.append(single_name)\n",
    "    # 作者\n",
    "    if authors != []:\n",
    "        xml_dict[x_doi]['authors'] = authors\n",
    "    keywords = []\n",
    "    key_find = tree.find('./front/article-meta/kwd-group')\n",
    "    if key_find != None:\n",
    "        for node in key_find:\n",
    "            if node.tag == 'kwd':\n",
    "                keyword = \"\".join(t for t in node.itertext()).replace('\\n', ' ')\n",
    "                keywords.append(keyword)\n",
    "        # 关键词\n",
    "        xml_dict[x_doi]['keywords'] = keywords\n",
    "    # 主标题\n",
    "    xml_dict[x_doi]['title'] = str_title\n",
    "    # 小标题及其序号\n",
    "    xml_dict[x_doi]['headings'] = headings\n",
    "    xml_dict[x_doi]['heading_idx'] = heading_idx\n",
    "    # 段落及其序号\n",
    "    xml_dict[x_doi]['paras'] = paras\n",
    "    xml_dict[x_doi]['para_idx'] = para_idx\n",
    "    \n",
    "# key=doi, 'title', 'headings', 'heading_idx', 'paras', 'para_index', 'abstract', 'time', 'authors', 'keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8114e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89046\n",
      "86614\n",
      "96850\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# 将abstracts中和xml重复的删去\n",
    "with open('xml_dict.json', 'r', encoding='utf-8') as f:\n",
    "    xml_dict = json.load(f) \n",
    "with open('passivating_abstract.json', 'r', encoding='utf-8') as f:\n",
    "    abstract_dict = json.load(f)\n",
    "print(len(abstract_dict.keys()))\n",
    "for i in xml_dict.keys():\n",
    "    if i in abstract_dict.keys():\n",
    "        del abstract_dict[i]\n",
    "print(len(abstract_dict.keys()))\n",
    "\n",
    "# 所有的abstract\n",
    "all_abstract = []\n",
    "for x in xml_dict.keys():\n",
    "    if 'abstract' in xml_dict[x].keys():\n",
    "        all_abstract.append(xml_dict[x]['abstract'])\n",
    "for i in abstract_dict.keys():\n",
    "    all_abstract.append(abstract_dict[i])\n",
    "print(len(all_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3e5f6",
   "metadata": {},
   "source": [
    "## Dictionary for Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40ead80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96850/96850 [54:43<00:00, 29.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "# 跑一遍abstracts准备一个把化合物全称变简称的词典 key=full_name, values = [abbs]\n",
    "from chemdataextractor import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "abb_dict = {}\n",
    "for i in tqdm(all_abstract):\n",
    "    abb = Document(i).abbreviation_definitions\n",
    "    if abb != []:\n",
    "        for tup in abb:\n",
    "            if tup[-1] == 'CM':\n",
    "                abb_name = ' '.join(tup[0])\n",
    "                quan_name = ' '.join(tup[1])\n",
    "                quan_name = quan_name.replace(' - ', '-')\n",
    "                if quan_name not in abb_dict.keys():\n",
    "                    abb_dict[quan_name] = [abb_name]\n",
    "                else:\n",
    "                    if abb_name not in abb_dict[quan_name]:\n",
    "                        abb_dict[quan_name].append(abb_name)\n",
    "# 该步结果存为json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79f3d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abb_dict.json', 'r', encoding='utf-8') as f:\n",
    "    abb_dict = json.load(f)\n",
    "    \n",
    "# 将键值对合并，[tmp_dict1, tmp_dict2], 每个dict['full_name':[], 'abb':[]]\n",
    "pair_list = []\n",
    "already = []\n",
    "for i, full in enumerate(list(abb_dict.keys())):\n",
    "    if i not in already:\n",
    "        full_list = [full]\n",
    "        abb_list = abb_dict[full]\n",
    "        already.append(i)\n",
    "        for abb in abb_dict[full]:\n",
    "            for j, value in enumerate(abb_dict.values()):\n",
    "                if abb in value and i!=j: \n",
    "                    full_list.append(list(abb_dict.keys())[j])\n",
    "                    for v in value:\n",
    "                        if v not in abb_list:\n",
    "                            abb_list.append(v)\n",
    "                    already.append(j)\n",
    "        tmp_dict = {'full':full_list, 'abb':abb_list}\n",
    "        pair_list.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b599f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于每一个简称，取下属囊括全称最长的作为normalized简称, 记作max_abb加入tmp_dict\n",
    "for d in pair_list:\n",
    "    if len(d['abb']) != 1:\n",
    "        count_dict={}\n",
    "        for abb in d['abb']:\n",
    "            count = 0\n",
    "            for value in abb_dict.values():\n",
    "                if abb in value:\n",
    "                    count += 1\n",
    "            count_dict[abb] = count\n",
    "        sort_count = sorted(count_dict.items(), key=lambda item:item[1], reverse=True)\n",
    "        max_abb = sort_count[0][0]\n",
    "        d['max_abb'] = max_abb  \n",
    "    else:\n",
    "        d['max_abb'] = d['abb'][0]\n",
    "        \n",
    "pair_list.sort(key= lambda x:len(x['full'][0]),reverse=True)\n",
    "# 该步结果存为pair_list.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41d0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize function\n",
    "def normalize(text, pairs):\n",
    "    for d in pairs:\n",
    "        for full in d['full']:\n",
    "            if full in text:\n",
    "                text = text.replace(' '+full+' ', ' '+d['max_abb']+' ')\n",
    "                \n",
    "        for abb in d['abb']:\n",
    "            kuo_abb = ' ('+abb+')'\n",
    "            if kuo_abb in text:\n",
    "                text = text.replace(kuo_abb, '')\n",
    "            if abb != d['max_abb']:                    \n",
    "                space_abb = ' '+abb+' '\n",
    "                if space_abb in text:\n",
    "                    text = text.replace(space_abb, ' '+d['max_abb']+' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742ad46",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c43d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def sen_seg(data):    \n",
    "    to_replace = ['et al. ', 'Fig. ', 'e.g. ', 'i.e. ', 'Ref. ', 'Figs. ', ' ca. ', 'approx. ', '(ca. ', 'etc.) ']\n",
    "    for tr in to_replace:\n",
    "        data = data.replace(tr, tr[:-2]+'####@')\n",
    "    tmp = nltk.sent_tokenize(data)\n",
    "    for i, t in enumerate(tmp):\n",
    "        for tr in to_replace:\n",
    "            t = t.replace(tr[:-2]+'####@', tr)\n",
    "        tmp[i] = t\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e642ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemdataextractor import Document\n",
    "\n",
    "def process_text(text, text_processor, pairs, max_abbs):\n",
    "    single_dict = {}        \n",
    "    text = normalize(text, pairs)                \n",
    "    ab_sens = sen_seg(text)\n",
    "    tup_0 = []\n",
    "    pro_sens = []\n",
    "    for s in ab_sens:\n",
    "        processed, _ = text_processor.process(s)\n",
    "        pro_sens.append(processed)\n",
    "        for tup in _:\n",
    "            # tup[0] is ori, tup[1] is normalized\n",
    "            if tup[0] not in tup_0:\n",
    "                tup_0.append(tup[0])\n",
    "            if tup[1] not in single_dict.keys():\n",
    "                single_dict[tup[1]] = {}\n",
    "                single_dict[tup[1]]['num'] = 1\n",
    "                single_dict[tup[1]]['ori'] = [tup[0]]\n",
    "            else:\n",
    "                single_dict[tup[1]]['num'] += 1 \n",
    "                if tup[0] not in single_dict[tup[1]]['ori']:\n",
    "                    single_dict[tup[1]]['ori'].append(tup[0])\n",
    "    for b in max_abbs:\n",
    "        if ' '+b+' ' in text and b not in tup_0:\n",
    "            abb_num = text.count(' '+b+' ')\n",
    "            single_dict[b] = {}\n",
    "            single_dict[b]['num'] = abb_num\n",
    "        \n",
    "    return pro_sens, single_dict      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8b28c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10320/10320 [1:25:21<00:00,  2.02it/s]\n",
      "100%|██████████| 86614/86614 [36:04<00:00, 40.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "from mat2vec.processing import MaterialsTextProcessor\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('pair_list.json', 'r', encoding='utf-8') as f:\n",
    "    pairs = json.load(f)\n",
    "print(len(pairs))\n",
    "# 4251 nomalization of abbrev\n",
    "# arrange normalization by length (long first)\n",
    "    \n",
    "# based dictionary of materials\n",
    "max_abbs = []\n",
    "for p in pairs:\n",
    "    max_abbs.append(p['max_abb'])\n",
    "\n",
    "# collect sentences, make mat dict\n",
    "sentences = []\n",
    "mat_dict = {}\n",
    "ori_dict = {}\n",
    "text_processor = MaterialsTextProcessor()\n",
    "judge_words = ['photovol', 'solar']\n",
    "for i in tqdm(xml_dict.keys()):\n",
    "    flag = 'N'\n",
    "    if 'abstract' not in xml_dict[i]:\n",
    "        judge = xml_dict[i]['title']\n",
    "    else:\n",
    "        judge = xml_dict[i]['abstract']\n",
    "    for j in judge_words:\n",
    "        if j in judge:\n",
    "            flag = 'Y'\n",
    "    texts = (' ').join(xml_dict[i]['paras'])\n",
    "    texts += ' '+xml_dict[i]['title']\n",
    "    sens, single_dict = process_text(texts, text_processor, pairs, max_abbs)\n",
    "    sentences.extend(sens)\n",
    "    for s in single_dict.keys():\n",
    "        if s in mat_dict.keys():\n",
    "            if flag in mat_dict[s]:\n",
    "                mat_dict[s][flag] += single_dict[s]['num']\n",
    "            else:\n",
    "                mat_dict[s][flag] = single_dict[s]['num']\n",
    "        else:\n",
    "            mat_dict[s]={}\n",
    "            mat_dict[s][flag] = single_dict[s]['num']\n",
    "    for s in single_dict.keys():\n",
    "        if 'ori' in single_dict[s].keys():\n",
    "            if s not in ori_dict.keys():\n",
    "                ori_dict[s] = single_dict[s]['ori']\n",
    "            else:\n",
    "                for o in single_dict[s]['ori']:\n",
    "                    if o not in ori_dict[s]:\n",
    "                        ori_dict[s].append(o)\n",
    "            \n",
    "for i in tqdm(abstract_dict.keys()):\n",
    "    flag = 'N'\n",
    "    texts = abstract_dict[i]\n",
    "    for j in judge_words:\n",
    "        if j in texts:\n",
    "            flag = 'Y'\n",
    "    sens, single_dict = process_text(texts, text_processor, pairs, max_abbs)\n",
    "    sentences.extend(sens)\n",
    "    for s in single_dict.keys():\n",
    "        if s in mat_dict.keys():\n",
    "            if flag in mat_dict[s]:\n",
    "                mat_dict[s][flag] += single_dict[s]['num']\n",
    "            else:\n",
    "                mat_dict[s][flag] = single_dict[s]['num']\n",
    "        else:\n",
    "            mat_dict[s]={}\n",
    "            mat_dict[s][flag] = single_dict[s]['num']\n",
    "    for s in single_dict.keys():\n",
    "        if 'ori' in single_dict[s].keys():\n",
    "            if s not in ori_dict.keys():\n",
    "                ori_dict[s] = single_dict[s]['ori']\n",
    "            else:\n",
    "                for o in single_dict[s]['ori']:\n",
    "                    if o not in ori_dict[s]:\n",
    "                        ori_dict[s].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54614a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(sentences, indent=4)\n",
    "with open('sentences.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)\n",
    "    \n",
    "json_str = json.dumps(ori_dict, indent=4)\n",
    "with open('ori_dict.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)\n",
    "    \n",
    "json_str = json.dumps(mat_dict, indent=4)\n",
    "with open('mat_dict.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605f28c",
   "metadata": {},
   "source": [
    "## Different configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b499f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('sentences.json', 'r', encoding='utf-8') as f:\n",
    "    sentences = json.load(f)\n",
    "with open('mat_dict.json', 'r', encoding='utf-8') as f:\n",
    "    mat_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad73f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# , sg=1\n",
    "model = Word2Vec(sentences, vector_size=200, window=8, min_count=4, workers=4, sg=1)\n",
    "model.save(\"skip_vec/word2vec_our.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55f9c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "cbow_model = Word2Vec.load('cbow_vec/word2vec_our.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8af6caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403285\n",
      "0.6740291\n",
      "0.67648166\n",
      "0.5696485\n",
      "0.25458235\n",
      "0.09250669\n",
      "0.5027627\n",
      "0.05856296\n",
      "-0.05344326\n"
     ]
    }
   ],
   "source": [
    "word_vectors = cbow_model.wv\n",
    "print(len(word_vectors.key_to_index))\n",
    "similarity = word_vectors.similarity('conduction', 'valence')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('PV', 'solar')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('PV', 'photovoltaics')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('contact', 'conduction')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('molecular', 'orbitals')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('PV', 'thermoelectric')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('conduction', 'deposition')\n",
    "print(similarity)\n",
    "similarity = word_vectors.similarity('water', 'piezoelectric')\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f65ed305",
   "metadata": {},
   "outputs": [],
   "source": [
    "passivation_sims = word_vectors.most_similar('passivation', topn=403285)\n",
    "contact_sims = word_vectors.most_similar('contact', topn=403285)\n",
    "conductivity_sims = word_vectors.most_similar('conductivity', topn=403285)\n",
    "\n",
    "passivation = word_vectors['passivation']\n",
    "contact = word_vectors['contact']\n",
    "conductivity = word_vectors['conductivity']\n",
    "combine_vec = (passivation + contact) / 2\n",
    "combine_sims = word_vectors.most_similar(positive=[combine_vec], topn=403285)\n",
    "combine_vec3 = (passivation + contact + conductivity) / 3\n",
    "combine_sims3 = word_vectors.most_similar(positive=[combine_vec3], topn=403285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1398f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('skip_cbm3.csv', 'w', encoding='utf-8',newline='') as csvfile:\n",
    "    f = csv.writer(csvfile)\n",
    "    f.writerow(['num','rank','name','freq_N', 'freq_Y'])\n",
    "    count = 1\n",
    "    for i, p_set in enumerate(combine_sims3):\n",
    "        p = p_set[0]\n",
    "        if p in mat_dict.keys(): \n",
    "            if 'N' in mat_dict[p].keys():\n",
    "                freq_N = mat_dict[p]['N']\n",
    "            else:\n",
    "                freq_N = 0\n",
    "            if 'Y' in mat_dict[p].keys():\n",
    "                freq_Y = mat_dict[p]['Y']\n",
    "            else:\n",
    "                freq_Y = 0\n",
    "            if freq_N + freq_Y >= 3:\n",
    "                f.writerow([str(count),str(i),p,str(freq_N), str(freq_Y)])\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77618114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
